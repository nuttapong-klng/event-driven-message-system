# Guideline: Autoscaling & High Availability (HA) for the Event-Driven Message System

This document provides practical steps and best practices to achieve autoscaling and high availability (HA) for your event-driven message system, based on the manifests in the `k8s/` directory.

---

## 1. General Principles

- **High Availability (HA):** Avoid single points of failure. Use multiple replicas, spread workloads across nodes, and use persistent storage.
- **Autoscaling:** Use Kubernetes Horizontal Pod Autoscaler (HPA) for stateless workloads, and cluster-aware/manual scaling for stateful components like Kafka and PostgreSQL.

---

## 2. Kafka (Strimzi Operator)

### High Availability

- **Brokers:** Set `replicas` for Kafka brokers to at least 3 in `02-kafka-cluster.yaml` for production HA.
- **Zookeeper:** Set Zookeeper replicas to at least 3.
- **Storage:** Use persistent volumes with replication and regular backups.
- **PodDisruptionBudgets:** Add PodDisruptionBudgets to ensure a minimum number of pods are always available during voluntary disruptions.

### Autoscaling

- **Kafka brokers do not support HPA** (due to stateful nature). Scale by increasing the `replicas` field in the Kafka custom resource.
- **Monitoring:** Use Strimzi's metrics and Prometheus to monitor broker load and plan scaling.

---

## 3. PostgreSQL

### High Availability

- **StatefulSet:** Use a StatefulSet with at least 2-3 replicas and configure streaming replication (primary/standby).
- **Operator:** Consider using a PostgreSQL operator (like Zalando or CrunchyData) for automated failover and backups.
- **Storage:** Use persistent volumes with backup and restore strategies.

### Autoscaling

- **Vertical Scaling:** Increase CPU/memory requests/limits as needed.
- **Horizontal Scaling:** Use read replicas for scaling reads (true horizontal scaling is complex for databases).

---

## 4. Backend & Frontend

### High Availability

- **Replicas:** In `04-backend.yaml` and `05-frontend.yaml`, set `replicas` in the Deployment spec to at least 2-3.
- **Readiness/Liveness Probes:** Ensure these are set so Kubernetes can detect and replace unhealthy pods.
- **PodDisruptionBudgets:** Add to ensure minimum availability during node maintenance.

### Autoscaling

- **Horizontal Pod Autoscaler (HPA):** Add an HPA resource for each Deployment:

```yaml
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
  namespace: event-driven-message-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
```

_Repeat for frontend and consumer deployments._

---

## 5. Node and Storage Considerations

- **Multi-node Cluster:** Run your cluster on at least 3 nodes for true HA.
- **Anti-affinity Rules:** Use `podAntiAffinity` to spread replicas across nodes.
- **StorageClass:** Use a StorageClass that supports replication and high availability.

---

## 6. Example: Adding HPA and PodDisruptionBudget

Add to your backend manifest (repeat for frontend/consumer):

```yaml
# Horizontal Pod Autoscaler
apiVersion: autoscaling/v2
kind: HorizontalPodAutoscaler
metadata:
  name: backend-hpa
  namespace: event-driven-message-system
spec:
  scaleTargetRef:
    apiVersion: apps/v1
    kind: Deployment
    name: backend
  minReplicas: 2
  maxReplicas: 10
  metrics:
    - type: Resource
      resource:
        name: cpu
        target:
          type: Utilization
          averageUtilization: 70
---
# PodDisruptionBudget
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata:
  name: backend-pdb
  namespace: event-driven-message-system
spec:
  minAvailable: 2
  selector:
    matchLabels:
      app: backend
```

---

## 7. Monitoring & Alerts

### Recommended Tools

- **Prometheus:** For metrics collection and alerting.
- **Grafana:** For visualization and dashboards.
- **Alertmanager:** For handling alerts generated by Prometheus.

### How to Deploy

1. **Add Prometheus and Grafana to Your Cluster:**

   - The easiest way is via the [kube-prometheus-stack Helm chart](https://github.com/prometheus-community/helm-charts/tree/main/charts/kube-prometheus-stack):
     ```bash
     helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
     helm repo update
     helm install monitoring prometheus-community/kube-prometheus-stack \
       --namespace monitoring --create-namespace
     ```
   - This will deploy Prometheus, Grafana, and Alertmanager in the `monitoring` namespace.

2. **Expose Metrics from Your Applications:**

   - Ensure your backend, frontend, and consumer applications expose Prometheus-compatible metrics endpoints (e.g., `/metrics`).
   - For Kafka, Strimzi provides ServiceMonitors and metrics configuration out of the box. Check the Strimzi documentation for enabling metrics.

3. **Configure ServiceMonitors:**

   - Create `ServiceMonitor` resources to tell Prometheus how to scrape your application metrics. Example:
     ```yaml
     apiVersion: monitoring.coreos.com/v1
     kind: ServiceMonitor
     metadata:
       name: backend-servicemonitor
       namespace: monitoring
     spec:
       selector:
         matchLabels:
           app: backend
       namespaceSelector:
         matchNames:
           - event-driven-message-system
       endpoints:
         - port: http
           path: /metrics
           interval: 30s
     ```
   - Repeat for each component (frontend, consumer, etc.).

4. **Dashboards:**

   - Use Grafana to import or create dashboards for Kafka, PostgreSQL, and your applications. The kube-prometheus-stack comes with many prebuilt dashboards.

5. **Alerts:**
   - Define alerting rules in Prometheus for key conditions, such as:
     - Pod not ready
     - High CPU/memory usage
     - Kafka broker down
     - PostgreSQL unavailable
   - Example alert rule:
     ```yaml
     apiVersion: monitoring.coreos.com/v1
     kind: PrometheusRule
     metadata:
       name: backend-alerts
       namespace: monitoring
     spec:
       groups:
         - name: backend.rules
           rules:
             - alert: BackendPodDown
               expr: kube_deployment_status_replicas_available{deployment="backend"} < 1
               for: 2m
               labels:
                 severity: critical
               annotations:
                 summary: "All backend pods are down"
                 description: "No backend pods are available in the event-driven-message-system namespace."
     ```
   - Configure Alertmanager to send notifications (email, Slack, etc.).

### Best Practices

- Regularly review and update alerting rules.
- Test alert delivery and escalation paths.
- Use dashboards to monitor system health and capacity trends.

---

## 8. Documentation & Testing

- Document your scaling and HA strategy.
- Regularly test failover and scaling in a staging environment.

---

## Summary Table

| Component  | HA Recommendation        | Autoscaling Approach    |
| ---------- | ------------------------ | ----------------------- |
| Kafka      | 3+ brokers, PDB, PV      | Manual (edit replicas)  |
| PostgreSQL | StatefulSet, operator    | Vertical, read replicas |
| Backend    | 2+ replicas, PDB, probes | HPA (CPU/memory)        |
| Frontend   | 2+ replicas, PDB, probes | HPA (CPU/memory)        |

---
